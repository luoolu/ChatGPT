{"pageProps":{"user":{"https://steamship.com/stripe":{"stripe_customer_id":"cus_NZmTUvDX5ARORZ"},"given_name":"Lu","family_name":"Luo","nickname":"argluolu","name":"Lu Luo","picture":"https://lh3.googleusercontent.com/a/AGNmyxY_7rTvhVE6664qYvuiC-EZHZYQatZLiB4sXVsH=s96-c","locale":"zh-CN","updated_at":"2023-03-23T01:25:59.022Z","email":"argluolu@gmail.com","email_verified":true,"sub":"google-oauth2|104529553697380159910","sid":"Tjw1yzb4QWk2xHDtP8wpBU9v1i2iRVdP","apiKey":"B9534D27-DDBB-4BFC-8185-87B44E4F9F15","handle":"argluolu","id":"FBE5A1BB-FF43-4075-8086-DB3AC7EF6DEE","handleSet":true},"invocableJson":{"id":"579981DC-888B-4EC5-A292-9697D6C9475B","description":null,"readme":null,"profile":{"configTemplate":{"presence_penalty":{"type":"number","description":"Control how likely the model will reuse words. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Number between -2.0 and 2.0.","default":0},"temperature":{"type":"number","description":"Controls randomness. Lower values produce higher likelihood / more predictable results; higher values produce more variety. Values between 0-1.","default":0.4},"max_tokens":{"type":"number","description":"The maximum number of tokens to generate per request. Can be overridden in runtime options.","default":256},"openai_api_key":{"type":"string","description":"An openAI API key to use. If left default, will use Steamship's API key.","default":""},"max_retries":{"type":"number","description":"Maximum number of retries to make when generating.","default":8},"default_system_prompt":{"type":"string","description":"System prompt that will be prepended before every request","default":""},"moderate_output":{"type":"boolean","description":"Pass the generated output back through OpenAI's moderation endpoint and throw an exception if flagged.","default":true},"n":{"type":"number","description":"How many completions to generate for each prompt.","default":1},"default_role":{"type":"string","description":"The default role to use for a block that does not have a Tag of kind='role'","default":"user"},"frequency_penalty":{"type":"number","description":"Control how likely the model will reuse words. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. Number between -2.0 and 2.0.","default":0},"request_timeout":{"type":"number","description":"Timeout for requests to OpenAI completion API. Default is 600 seconds.","default":600},"top_p":{"type":"number","description":"Controls the nucleus sampling, where the model considers the results of the tokens with top_p probability mass. Values between 0-1.","default":1},"model":{"type":"string","description":"The OpenAI model to use.  Can be a pre-existing fine-tuned model.","default":"gpt-4"}},"steamshipRegistry":{"authorGithub":"dkolas","authorEmail":"developers@steamship.com","tags":["GPT-4","Prompt Completion","LLM","GPT","OpenAI"],"tagline":"Complete prompts and chats with GPT-4","authorName":"dkolas"},"author":"dave","handle":"gpt-4","type":"plugin","version":"0.0.1-rc.4","public":true,"description":"","plugin":{"isTrainable":false,"transport":"jsonOverHttp","type":"generator"}},"handle":"gpt-4","type":"generator"}},"__N_SSP":true}
